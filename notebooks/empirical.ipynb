{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tournament as tn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/alta2016.csv', 'data/asu2016.csv', 'data/berk2016.csv', 'data/blake2016.csv', 'data/emery2016.csv', 'data/fullerton2016.csv', 'data/georgetown2016.csv', 'data/glenbrooks2016.csv', 'data/goldendesert2016.csv', 'data/gonzaga2016.csv', 'data/ndca2016.csv', 'data/toc2016.csv']\n"
     ]
    }
   ],
   "source": [
    "print(glob.glob(\"data/*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Estimating real distributions of skill\n",
    "\n",
    "We can find parameters that allow our distributions to fit the given data well. We do this via maximum likelihood estimation (MLE) and log-likelihood calculation, a procedure well-established in the literature.\n",
    "\n",
    "Formally, we say:\n",
    "\n",
    "$$LL_x = N_x \\times \\ln(P\\hat (X=x))$$\n",
    "\n",
    "and \n",
    "\n",
    "$$LL = \\sum_{x} LL_x$\n",
    "\n",
    "Here, $x$ represents the number of wins; i.e. between 0 and 6 in most specifications. Then, $N_x$ is the number of teams in the real tournament which have $x$ wins. \n",
    "\n",
    "We present a case using data from the 2016 [National Debate Coaches Association Championship](http://www.debatecoaches.org/). Here, 68 teams debated 6 rounds each. The distribution is shown below.\n",
    "\n",
    "**CHART/HISTOGRAM GOES HERE**\n",
    "\n",
    "Then, we choose to model team skill using a beta distribution. This distribution is well established in the literature for its flexibility and ease of computation. The beta distribution is defined on the interval [0, 1] and takes two shape parameters, which we will call $\\alpha$ and $\\beta$. Then, we perform optimization on $\\alpha$ and $\\beta$ to find the best combination of parameters which fits the observed data.\n",
    "\n",
    "Although the hypergeometric distribution more closely resembles the 'story,' as a team has 6 chances to pick samples from 6 wins and 6 losses, the hypergeometric does not take into account the [...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2\n",
       "1     8\n",
       "2    16\n",
       "3    16\n",
       "4    19\n",
       "5     5\n",
       "6     2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndca_summ = pd.read_csv(\"data/ndca_summ.csv\")\n",
    "ndca_summ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_hat = 2.0\n",
    "beta_hat = 5.0\n",
    "ndca = tn.Simulation(68, 6, 100, **{'dist': 'beta', 'shape1': alpha_hat, 'shape2': beta_hat})\n",
    "ndca.simulate()\n",
    "y_hat = ndca.get_results().wins.value_counts(normalize=True, sort=True).sort_index()\n",
    "logLikelihood = -sum(np.log(y_hat) * ndca_summ['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimateLL(params):\n",
    "    alpha_hat = params[0]\n",
    "    beta_hat = params[1]\n",
    "    t = tn.Simulation(68, 6, 20, **{'dist': 'beta', 'shape1': alpha_hat, 'shape2': beta_hat})\n",
    "    t.simulate()\n",
    "    y_hat = t.get_results().wins.value_counts(normalize=True, sort=True).sort_index()\n",
    "    log_like = -sum(np.log(y_hat) * ndca_summ['count'])\n",
    "    return log_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "init_params = [2.0, 5.0]\n",
    "# results = minimize(estimateLL, init_params, method='L-BFGS-B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.571463809\n",
      "116.68325255\n",
      "116.418556031\n",
      "116.427861262\n",
      "116.020138079\n",
      "118.060026772\n",
      "116.476572474\n"
     ]
    }
   ],
   "source": [
    "print(estimateLL([2.0,5.0]))\n",
    "print(estimateLL([1.0,5.0]))\n",
    "print(estimateLL([1.5,5.0]))\n",
    "print(estimateLL([1.0,0.5]))\n",
    "print(estimateLL([1.5,0.5]))\n",
    "print(estimateLL([1.5,1.0]))\n",
    "print(estimateLL([1.5,1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateLLExp(params):\n",
    "    alpha_hat = params[0]\n",
    "    ndca = tn.Simulation(68, 6, 20, **{'dist': 'exp', 'sc': alpha_hat})\n",
    "    ndca.simulate()\n",
    "    y_hat = ndca.get_results().wins.value_counts(normalize=True, sort=True).sort_index()\n",
    "    log_like = -sum(np.log(y_hat) * ndca_summ['count'])\n",
    "    return log_like\n",
    "\n",
    "init_params2 = [0.5]\n",
    "# results2 = minimize(estimateLLExp, init_params2, method='L-BFGS-B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.37502362\n",
      "121.058850217\n",
      "119.623935197\n",
      "117.987665343\n",
      "120.897590488\n",
      "120.909639898\n",
      "119.90027864\n",
      "118.657911614\n"
     ]
    }
   ],
   "source": [
    "print(estimateLLExp([0.1]))\n",
    "print(estimateLLExp([0.3]))\n",
    "print(estimateLLExp([0.5]))\n",
    "print(estimateLLExp([0.7]))\n",
    "print(estimateLLExp([1.0]))\n",
    "print(estimateLLExp([1.5]))\n",
    "print(estimateLLExp([2.0]))\n",
    "print(estimateLLExp([2.5]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
